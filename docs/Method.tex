\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage[left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\renewcommand{\baselinestretch}{1.15}
\DeclareMathOperator{\Tr}{Tr}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bX}{\mathbf{X}}

\author{Bharath Ananthasubramaniam}
\title{Circular coupled (sparse) PCA}
\begin{document}
\maketitle
This approach aims to find two loading vectors (LVs) such that the projection of the data matrix  on these two directions maximizes the projected variance and in addition principal components (PCs) satisfy an elliptical constraint.\todo{Is this better formulated/called a type of SVD or a type of PCA.}

We now formalize this idea. The data of interest is $\textbf{X}$, a $n\times p$ matrix with $p$ features and $n$ samples. The coupled LVs are $\textbf{v}_1$ and $\textbf{v}_2$, each a $p\times 1$ vector, and the corresponding PCs are $\textbf{u}_1$ and $\textbf{u}_2$, each a $n \times 1$ vector. To obtain a variance interpretation for the projection, the data matrix $\textbf{X}$ is column-centered, $\sum_j X_{ij} = 0$.

\subsection*{The general problem}

We can formulate our approach as a reduced rank approximation similar to singular value decomposition, but with two key differences. First, we search for coupled PCs/SVs (two PCs simultaneously) that satisfy an elliptical constraint. Second, we do not enforce orthogonality of the two PCs.

\begin{align*}
\min_{\mathbf{u}_1, \mathbf{v}_1, \mathbf{u}_1, \mathbf{v}_1} \| \mathbf{X} &- \mathbf{u}_1\mathbf{v}_1^T - \mathbf{u}_2\mathbf{v}_2^T\|_F^2 = \\
  &= \min_{\mathbf{u}_1, \mathbf{v}_1, \mathbf{u}_1, \mathbf{v}_1} \Tr \left((\mathbf{X} - \mathbf{u}_1\mathbf{v}_1^T - \mathbf{u}_2\mathbf{v}_2^T)^T~(\mathbf{X} - \mathbf{u}_1\mathbf{v}_1^T - \mathbf{u}_2\mathbf{v}_2^T) \right)\\
  &= \min_{\mathbf{u}_1, \mathbf{v}_1, \mathbf{u}_1, \mathbf{v}_1} \Tr \left[\bX^T\bX - \bX^T \bu_1 \bv_1^T - \bX^T \bu_2\bv_2^T - \bv_1\bu_1^T\bX + \bv_1\bu_1^T \bu_1\bv_1^T + \bv_1\bu_1^T \bu_2\bv_2^T \right. \\
  & \qquad \qquad\qquad \left. -\bv_2\bu_2^T\bX + \bv_2\bu_2^T \bu_1 \bv_1^T + \bv_2\bu_2^T \bu_2\bv_2^T\right]\\
  &= \max_{\mathbf{u}_1, \mathbf{v}_1, \mathbf{u}_1, \mathbf{v}_1} 2\bu_1^T\bX\bv_1 + 2\bu_2^T\bX\bv_2 - \|\bu_1\|_2^2 \|\bv_1\|_2^2 - \|\bu_2\|_2^2 \|\bv_2\|_2^2 - 2\bu_1^T\bu_2\bv_1^T\bv_2.
\end{align*}
We enforce a $l_2$-constraint on the LVs ($\|\bv_1\|_2 = \|\bv_2\|_2 = 1$) and the elliptical constraint (${\bu_1^2}_i + {\bu_2^2}_i = 1~ \forall i\in \{1,\ldots,n\}$).

\begin{equation}
\max \left\{\mathbf{u}_1^T \mathbf{X} \mathbf{v}_1 + \mathbf{u}_2^T \mathbf{X} \mathbf{v}_2 - \bu_1^T\bu_2\bv_1^T\bv_2\right\} ~\textrm{s.t.}~ \|\mathbf{v}_1\|_2 = \|\mathbf{v}_2\|_2=1,~ {\mathbf{u}_1^2}_i + {\mathbf{u}_2^2}_i = 1~ \forall i\in \{1,\ldots,n\}
\label{eqn:generalproblem}
\end{equation}

This transformation maps the data into the best possible approximate ellipse and the cost function maximizes the sum of the major and minor axis of this ellipse (note the sum is invariant to rotations of the ellipse).

\subsubsection*{Solving this optimization}
This optimization is hard to solve; we need to either enforce orthogonality between the LVs or PCs, or optimize this coupled cost function. We simplify this problem \todo{better way to justify this?} by keeping only the first two terms in this optimization. 
\begin{equation}
\max \left\{\mathbf{u}_1^T \mathbf{X} \mathbf{v}_1 + \mathbf{u}_2^T \mathbf{X} \mathbf{v}_2 \right\} ~\textrm{s.t.}~ \|\mathbf{v}_1\|_2 = \|\mathbf{v}_2\|_2=1,~ {\mathbf{u}_1^2}_i + {\mathbf{u}_2^2}_i = 1~ \forall i\in \{1,\ldots,n\}
\end{equation}

This simplified equation is bi-convex in $\textbf{u}$s and $\textbf{v}$s \todo{how do we prove this?} and can be solved using alternate maximization.

Given possible $\mathbf{v}_1$, $\mathbf{v}_2$, we can define dummy variables $\mathbf{y}_1 = \mathbf{X}\mathbf{v}_1$ and $\mathbf{y}_2 = \mathbf{X}\mathbf{v}_2$. The maximization for the $\mathbf{u}$s can be carried out for each $i$ independently. We can maximize the cost function in \eqref{eqn:generalproblem} by choosing
\begin{equation}
{\mathbf{u}_1}_i = \frac{{\mathbf{y}_1}_i}{\sqrt{{\mathbf{y}_1^2}_i + {\mathbf{y}_2^2}_i}} ~~\textrm{and}~~ {\mathbf{u}_2}_i = \frac{{\mathbf{y}_2}_i}{\sqrt{{\mathbf{y}_1^2}_i + {\mathbf{y}_2^2}_i}}
\label{eqn:solve_u}
\end{equation}

Given now a solution from \eqref{eqn:solve_u}, the solutions for $\mathbf{v}$s that maximize the cost function can be computed independently simply as
\begin{equation}
 \mathbf{v}_1 = \frac{\mathbf{X}^T\mathbf{u}_1}{\|\mathbf{X}^T\mathbf{u}_1\|_2} ~~\textrm{and}~~ \mathbf{v}_2 = \frac{\mathbf{X}^T\mathbf{u}_2}{\|\mathbf{X}^T\mathbf{u}_2\|_2}
 \label{eqn:solve_v}
 \end{equation} 
 
 We iterate \eqref{eqn:solve_u} and \eqref{eqn:solve_v} from a suitable random initial choice of vectors until convergence of $\mathbf{u}$s and $\mathbf{v}$s.
 
\subsection*{The sparse problem}
We can obtain more interpretable solutions by constraining the LVs to be suitably sparse. This introduces a hyperparameter $t$ controlling the sparsity of $\mathbf{v}$s.

\begin{multline}
\max \left\{ \mathbf{u}_1^T \mathbf{X} \mathbf{v}_1 + \mathbf{u}_2^T \mathbf{X} \mathbf{v}_2\right\} \\~\textrm{s.t.}~ \|\mathbf{v}_1\|_2 = \|\mathbf{v}_2\|_2=1,~\|\mathbf{v}_1\|_1\leq t, \|\mathbf{v}_2\|_1\leq t,~ {\mathbf{u}_1^2}_i + {\mathbf{u}_2^2}_i = 1~ \forall i\in \{1,\ldots,n\}
\label{eqn:sparseproblem}
\end{multline}

\subsubsection*{Solving this optimization}
This optimization \eqref{eqn:sparseproblem} is solved in a similar manner to the general problem \eqref{eqn:generalproblem}. Only the update equation \eqref{eqn:solve_v} for the $\mathbf{v}$s differ as we also have to enforce the sparsity constraint. We solve the joint $\ell_1$ and $\ell_2$ constraint in a single step using the approach in Guillemot et al. (2019).



\end{document}